{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae41fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from eagle import EagleRanker, ModelScore\n",
    "from eval import compute_accuracy\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(\"notdiamond/repliqa_gpt4o_gpt4omini_evals\")\n",
    "ds_train = ds[\"train\"].to_pandas().sample(n=1000, random_state=42)\n",
    "ds_val = ds[\"val\"].to_pandas().sample(n=100, random_state=42)\n",
    "ds_test = ds[\"test\"].to_pandas().sample(n=100, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5b11ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EagleRanker with training data\n",
    "eagle_ranker = EagleRanker(\n",
    "    prompts=list(ds_train[\"prompt\"]),\n",
    "    model_scores = {\n",
    "        \"gpt-4o-mini-2024-07-18\": list(ds_train[\"gpt-4o-mini-2024-07-18/score\"]),\n",
    "        \"gpt-4o-2024-08-06\": list(ds_train[\"gpt-4o-2024-08-06/score\"])\n",
    "    },\n",
    "    embedding_filepath=\"embeddings.json\",\n",
    "    k=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7054dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eagle_ranker: EagleRanker, dataset: pd.DataFrame, p: float, print_flags: bool = False):\n",
    "    # Compute test set performance on original p\n",
    "    ranked_scores = list[list[ModelScore]]()\n",
    "    for _, row in dataset.iterrows():\n",
    "        model_scores = eagle_ranker.rank(\n",
    "                text=row[\"prompt\"],\n",
    "                n=20,\n",
    "                p=p\n",
    "            )\n",
    "        ranked_scores.append(\n",
    "            model_scores\n",
    "        )\n",
    "    accuracy = compute_accuracy(\n",
    "        model_ground_truth_scores={\n",
    "            \"gpt-4o-mini-2024-07-18\": list(dataset[\"gpt-4o-mini-2024-07-18/score\"]),\n",
    "            \"gpt-4o-2024-08-06\": list(dataset[\"gpt-4o-2024-08-06/score\"])\n",
    "        },\n",
    "        ranker_scores=ranked_scores,\n",
    "        print_flags=print_flags,\n",
    "    )\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20df9f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p=0.00, accuracy=0.1400\n",
      "p=0.10, accuracy=0.1600\n",
      "p=0.20, accuracy=0.1500\n",
      "p=0.30, accuracy=0.1400\n",
      "p=0.40, accuracy=0.1600\n",
      "p=0.50, accuracy=0.1600\n",
      "p=0.60, accuracy=0.1600\n",
      "p=0.70, accuracy=0.1600\n",
      "p=0.80, accuracy=0.1600\n",
      "p=0.90, accuracy=0.1600\n",
      "p=1.00, accuracy=0.1600\n"
     ]
    }
   ],
   "source": [
    "# Find the optimal value for p using the validation set\n",
    "for p in np.linspace(0, 1, 11):\n",
    "    accuracy = evaluate(eagle_ranker, ds_val, p, print_flags=False)\n",
    "    print(f\"p={p:.2f}, accuracy={accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f323f43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini-2024-07-18 gpt-4o-2024-08-06\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "gpt-4o-mini-2024-07-18 gpt-4o-mini-2024-07-18\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "gpt-4o-mini-2024-07-18 gpt-4o-mini-2024-07-18\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "gpt-4o-mini-2024-07-18 gpt-4o-mini-2024-07-18\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "gpt-4o-mini-2024-07-18 gpt-4o-mini-2024-07-18\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "gpt-4o-mini-2024-07-18 gpt-4o-2024-08-06\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "gpt-4o-mini-2024-07-18 gpt-4o-2024-08-06\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "gpt-4o-mini-2024-07-18 gpt-4o-2024-08-06\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "gpt-4o-mini-2024-07-18 gpt-4o-mini-2024-07-18\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "gpt-4o-mini-2024-07-18 gpt-4o-mini-2024-07-18\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "gpt-4o-mini-2024-07-18 gpt-4o-mini-2024-07-18\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "gpt-4o-mini-2024-07-18 gpt-4o-2024-08-06\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "gpt-4o-mini-2024-07-18 gpt-4o-mini-2024-07-18\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "Same scores but different rank\n",
      "accuracy=0.0800\n"
     ]
    }
   ],
   "source": [
    "# Compute test set performance on original p\n",
    "accuracy = evaluate(eagle_ranker, ds_test, p=0.5, print_flags=True)\n",
    "print(f\"accuracy={accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9368572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.0800\n"
     ]
    }
   ],
   "source": [
    "# Compute test set performance on validated p\n",
    "# I'm just using 1, validation doesn't give us better results than the default\n",
    "# but if we use 1 that means we can get away with just using the global which means less computation\n",
    "accuracy = evaluate(eagle_ranker, ds_test, p=1, print_flags=False)\n",
    "print(f\"accuracy={accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995204b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eagle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
